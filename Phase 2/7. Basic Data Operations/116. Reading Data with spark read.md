# 116. Reading Data with spark read

Canonical documentation for 116. Reading Data with spark read. This document defines the conceptual model, terminology, and standard usage patterns.

> [!NOTE]
> This documentation is implementation-agnostic and intended to serve as a stable reference.

## 1. Purpose and Problem Space
Describe why 116. Reading Data with spark read exists and the class of problems it addresses.
The `spark.read` module exists to provide a unified interface for reading data from various sources into Apache Spark DataFrames. The primary problem it addresses is the need for a flexible, efficient, and scalable way to load data from different formats and locations, such as CSV, JSON, Parquet, and JDBC databases, into Spark for processing and analysis.

## 2. Conceptual Overview
Provide a high-level mental model of the topic.
The `spark.read` module is part of the Apache Spark API, which enables data engineers and data scientists to read data from various sources into Spark DataFrames. This module provides a simple, intuitive, and efficient way to load data, handling complexities such as data type inference, schema detection, and data partitioning.

## 3. Terminology and Definitions
| Term | Definition |
|------|------------|
| DataFrame | A distributed collection of data organized into named columns, similar to a table in a relational database. |
| Data Source | The origin of the data, such as a file, database, or other data storage system. |
| Format | The structure and organization of the data, such as CSV, JSON, or Parquet. |
| Schema | The definition of the structure of the data, including column names and data types. |
| Partitioning | The process of dividing data into smaller, more manageable pieces, called partitions, to improve processing efficiency. |

## 4. Core Concepts
Explain the fundamental ideas that form the basis of this topic.
The core concepts of `spark.read` include:
* **Data Ingestion**: The process of reading data from a data source into a Spark DataFrame.
* **Format Handling**: The ability to read data from various formats, such as CSV, JSON, and Parquet.
* **Schema Inference**: The automatic detection of the schema of the data, including column names and data types.
* **Partitioning**: The division of data into smaller pieces to improve processing efficiency.

## 5. Standard Model
Describe the generally accepted or recommended model.
The standard model for using `spark.read` involves the following steps:
1. Create a SparkSession, which is the entry point to programming Spark.
2. Use the `read` method of the SparkSession to create a DataFrameReader.
3. Specify the format of the data using the `format` method.
4. Specify the data source using the `load` method.
5. Optionally, specify the schema of the data using the `schema` method.

## 6. Common Patterns
Document recurring, accepted patterns.
Common patterns when using `spark.read` include:
* **Reading CSV files**: Using the `format("csv")` method to read CSV files into a DataFrame.
* **Reading JSON files**: Using the `format("json")` method to read JSON files into a DataFrame.
* **Reading Parquet files**: Using the `format("parquet")` method to read Parquet files into a DataFrame.
* **Specifying a schema**: Using the `schema` method to specify the schema of the data.

## 7. Anti-Patterns
Describe common but discouraged practices.
Anti-patterns when using `spark.read` include:
* **Not specifying the format**: Failing to specify the format of the data, which can lead to errors or incorrect data types.
* **Not specifying the schema**: Failing to specify the schema of the data, which can lead to incorrect data types or missing columns.
* **Using the wrong format**: Using the wrong format, such as reading a CSV file as JSON, which can lead to errors or incorrect data.

## 8. References
Provide exactly five authoritative external references.
1. [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
2. [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)
3. [DataFrames and Datasets Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html#dataframes-and-datasets)
4. [Spark Read API Documentation](https://spark.apache.org/docs/latest/api/java/org/apache/spark/sql/DataFrameReader.html)
5. [Spark Tutorial: Reading and Writing Data](https://spark.apache.org/docs/latest/sql-programming-guide.html#reading-and-writing-data)

## 9. Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial documentation |