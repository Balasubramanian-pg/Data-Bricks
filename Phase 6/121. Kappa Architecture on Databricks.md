# 121. Kappa Architecture on Databricks

Canonical documentation for [121. Kappa Architecture on Databricks](Phase 6/121. Kappa Architecture on Databricks.md). This document defines concepts, terminology, and standard usage.

## Purpose
The Kappa Architecture on Databricks addresses the complexity inherent in traditional dual-path data processing systems (Lambda Architecture). By treating all data—whether real-time or historical—as a continuous stream, it eliminates the need to maintain separate codebases for batch and speed layers. This topic exists to define how the Databricks Lakehouse Platform facilitates a unified processing model where the same logic is applied to both live data ingestion and historical data reprocessing.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural paradigm rather than specific API syntax.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * The theoretical transition from Lambda to Kappa within a Lakehouse environment.
> * The role of unified streaming and batch engines.
> * Data persistence strategies for stream replay.
> * Consistency and idempotency requirements.

> [!WARNING]
> **Out of scope:**
> * Specific cloud provider infrastructure (e.g., AWS Kinesis vs. Azure Event Hubs configuration).
> * Detailed performance tuning of specific Spark clusters.
> * Comparison with non-Lakehouse streaming platforms.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Kappa Architecture | A software architecture pattern that handles all data processing via a single stream-processing engine, treating historical data as a stream replay. |
| Structured Streaming | A scalable and fault-tolerant stream processing engine that treats a live stream as a table that is being continuously appended. |
| Delta Lake | An open-source storage layer that brings ACID transactions to big data workloads, serving as the "long-term log" for Kappa Architecture. |
| Medallion Architecture | A data design pattern (Bronze, Silver, Gold) used to organize data within a Lakehouse, often serving as the physical implementation of Kappa. |
| Checkpointing | A mechanism that stores the state of a streaming query to ensure fault tolerance and exactly-once processing. |
| Idempotency | The property of an operation where it can be applied multiple times without changing the result beyond the initial application. |

## Core Concepts
The Kappa Architecture on Databricks is built upon the principle that a sufficiently fast and scalable stream processing engine can handle both real-time updates and historical re-computations.

**1. The Unified Log**
In this model, the "source of truth" is an immutable, append-only log. In the Databricks context, this is typically a combination of a message queue (for transient data) and Delta Lake (for long-term retention).

**2. Stream-Batch Unification**
The processing engine does not distinguish between a "batch" of files and a "stream" of messages. It views both as a sequence of data points. This allows developers to write logic once and apply it to any data regardless of its arrival time.

**3. Replayability**
To handle logic changes or data corruption, the architecture relies on the ability to "replay" the stream from a specific point in time. Because Delta Lake stores historical versions of data, it acts as the permanent record from which the stream can be re-initialized.

> [!TIP]
> Think of Kappa Architecture like a DVR for your data. If you need to change how you "watched" the data (i.e., change your business logic), you simply rewind the tape to the beginning and play it back through the new logic.

## Standard Model
The standard model for Kappa Architecture on Databricks follows the **Medallion Architecture** powered by **Structured Streaming**.

1.  **Ingestion (Bronze):** Raw data is captured from source systems (Kafka, IoT Hubs, Cloud Storage) and persisted into a "Bronze" Delta table. This table acts as the permanent, immutable log of all incoming events.
2.  **Refinement (Silver):** A streaming process reads from the Bronze table, applies transformations, cleanses the data, and writes to a "Silver" table. This is where the primary business logic resides.
3.  **Aggregation (Gold):** A final streaming or incremental process reads from Silver to create "Gold" tables—highly aggregated, business-ready datasets optimized for analytics and ML.

In this model, if the business logic in the Silver layer changes, the developer simply clears the Silver and Gold tables and restarts the stream from the Bronze table's inception.

## Common Patterns
*   **Change Data Capture (CDC) Integration:** Using streaming to ingest database logs and reconstruct state in Delta Lake using `MERGE` operations.
*   **Continuous Machine Learning:** Features are updated in real-time as data flows through the Silver layer, ensuring that model inference always uses the most recent data.
*   **Time-Travel Re-processing:** Leveraging Delta Lake's versioning to re-run streaming pipelines against a specific "snapshot" of the past to validate logic changes.

## Anti-Patterns
*   **Dual Logic Maintenance:** Maintaining one SQL script for "historical backfills" and a separate Streaming Python script for "real-time." This defeats the purpose of Kappa.
*   **Lack of Checkpointing:** Running streams without persistent checkpoints, leading to data loss or duplicates upon cluster restart.
*   **Over-reliance on Transient Storage:** Treating the message broker (e.g., Kafka) as the only source of truth. Message brokers usually have short retention periods; Delta Lake should be the long-term log.

> [!CAUTION]
> Avoid "Hard-Coding" offsets. Relying on manual offset management instead of the engine's native checkpointing leads to brittle pipelines and potential data integrity issues during failover.

## Edge Cases
*   **Late-Arriving Data:** Data that arrives after the "watermark" has passed. In Kappa, this requires defined policies on whether to drop the data or update historical aggregates.
*   **Schema Evolution:** When the source data format changes. Databricks provides schema enforcement and evolution, but breaking changes in the stream require careful "restart-from-beginning" strategies.
*   **Massive Historical Backfills:** When replaying years of data, the resource requirements may spike significantly compared to daily "steady-state" streaming.

## Related Topics
*   **Delta Live Tables (DLT):** A declarative framework for building reliable, maintainable, and testable data processing pipelines.
*   **Structured Streaming:** The underlying engine for stream processing in Apache Spark.
*   **ACID Transactions:** The fundamental guarantee provided by Delta Lake that enables concurrent streaming reads and writes.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |