# 138. Custom Data Quality Logic

Canonical documentation for [138. Custom Data Quality Logic](Phase 6/138. Custom Data Quality Logic.md). This document defines concepts, terminology, and standard usage.

## Purpose
Custom Data Quality (DQ) Logic exists to bridge the gap between generic data validation (such as null checks or data type verification) and domain-specific business requirements. While standard data quality tools provide foundational checks, they cannot account for the unique semantic relationships, industry-specific regulations, or complex cross-functional dependencies inherent in specialized datasets.

This topic addresses the need for a framework that allows organizations to define, execute, and manage proprietary validation rules that ensure data is not only technically correct but also contextually fit for purpose.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural and logical requirements rather than specific software syntax.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * Definition and categorization of custom validation rules.
> * Logical frameworks for multi-row and multi-table validation.
> * Threshold management and severity classification.
> * Integration of business context into automated quality pipelines.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Great Expectations, Deequ, or Informatica syntax).
> * General data governance policies (e.g., data stewardship roles).
> * Hardware-level performance tuning for data processing.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Custom Rule | A user-defined logical expression that evaluates data against specific business criteria. |
| Threshold | The acceptable limit of failure (percentage or count) before a data quality check is considered a failure. |
| Row-Level Logic | Validation that occurs within a single record, independent of other records. |
| Set-Level Logic | Validation that requires scanning multiple records or tables to determine quality (e.g., aggregations). |
| Deterministic Rule | A rule that always produces the same output given the same input data. |
| Probabilistic Rule | A rule that uses statistical models or heuristics to identify anomalies rather than binary pass/fail logic. |

## Core Concepts
The fundamental ideas of custom data quality logic revolve around the transition from "syntactic correctness" to "semantic integrity."

**1. Declarative vs. Imperative Logic**
Custom DQ logic can be defined declaratively (stating *what* the data should look like) or imperatively (stating *how* to check the data). Declarative approaches are generally preferred for maintainability and auditability.

**2. Validation Dimensions**
Custom logic typically extends the standard dimensions of data quality:
*   **Accuracy:** Does the data reflect the real-world entity? (e.g., "Is this GPS coordinate in the ocean?")
*   **Consistency:** Does the data match across disparate systems? (e.g., "Is the status in the CRM the same as the status in the Billing system?")
*   **Reasonableness:** Does the value fall within a logically sound range for its specific context?

> [!TIP]
> Think of standard DQ as a "spell checker" and custom DQ as a "fact checker." A sentence can be spelled correctly but still be factually incorrect or logically inconsistent.

## Standard Model
The standard model for implementing custom DQ logic follows a cyclical lifecycle:

1.  **Requirement Discovery:** Identifying the business risk associated with specific data points.
2.  **Logic Specification:** Defining the mathematical or logical expression (e.g., `IF status == 'Active' THEN payment_method NOT NULL`).
3.  **Execution Layer:** Applying the logic during data ingestion (In-flight), storage (At-rest), or consumption (On-read).
4.  **Evaluation & Scoring:** Comparing the results against defined thresholds.
5.  **Incident Response:** Triggering alerts, quarantining data, or initiating automated remediation.

## Common Patterns
*   **Cross-Field Dependency:** Validating one field based on the value of another (e.g., if `Country` is 'USA', then `ZipCode` must match a 5-digit or 9-digit format).
*   **Temporal Sequence:** Ensuring events occur in a logical chronological order (e.g., `OrderDate` must be before `ShipDate`).
*   **Reference Integrity (Custom):** Validating against a dynamic or external reference set that is not enforced by database constraints.
*   **Statistical Profiling:** Using historical means and standard deviations to detect outliers in new batches of data.

## Anti-Patterns
*   **Hardcoding Logic in ETL:** Embedding complex business rules directly into transformation code, making them invisible to data stewards and difficult to update.
*   **Over-Validation:** Creating so many custom rules that "alert fatigue" occurs, leading to critical failures being ignored.
*   **Circular Dependencies:** Defining rules that rely on the output of other rules in a way that creates infinite loops or ambiguous results.
*   **Ignoring Nulls in Logic:** Writing custom logic that fails to account for null values, often resulting in "False Passes" or runtime errors.

> [!CAUTION]
> Avoid tight coupling between the DQ logic and the physical storage schema. If the schema changes, the logic should ideally remain valid or be easily portable.

## Edge Cases
*   **Late-Arriving Data:** Rules that validate completeness may fail if data arrives across different time windows, requiring "look-back" logic.
*   **Schema Evolution:** When a source system adds or changes fields, custom logic may become obsolete or cause failures if not designed for flexibility.
*   **High-Cardinality Comparisons:** Performing set-level validation on billions of rows can lead to significant performance degradation, requiring sampled validation or incremental checks.
*   **Multi-Language/Locale Data:** Logic that assumes a specific format (e.g., date formats or currency symbols) may fail when applied to global datasets.

## Related Topics
*   **102. Data Observability:** The broader practice of monitoring system health, of which custom DQ is a component.
*   **045. Metadata Management:** Storing the definitions and results of custom DQ rules.
*   **210. Automated Remediation:** The process of fixing data errors identified by custom logic.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |