# 104. Result Caching

Canonical documentation for [104. Result Caching](Phase 6/104. Result Caching.md). This document defines concepts, terminology, and standard usage.

## Purpose
Result Caching is a performance optimization strategy designed to reduce latency and computational overhead by storing the output of an operation and reusing it for subsequent requests with identical inputs. This topic addresses the problem of redundant processing in environments where data retrieval or computation is significantly more expensive (in terms of time, CPU, or cost) than the overhead of maintaining a storage layer.

The primary objective is to decouple the frequency of execution from the frequency of consumption, ensuring that expensive operations are performed only when necessary.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural principles of result caching rather than specific software libraries.

## Scope
This documentation covers the logical frameworks, lifecycle management, and structural patterns associated with caching the outputs of functions, queries, and service calls.

> [!IMPORTANT]
> **In scope:**
> * Core functionality: Cache keys, TTL management, and hit/miss logic.
> * Theoretical boundaries: Determinism, idempotency, and state consistency.
> * Lifecycle: Invalidation strategies and eviction policies.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Redis, Memcached, Ehcache configurations).
> * Hardware-level caching (e.g., L1/L2 CPU caches).
> * Browser-specific DOM caching mechanisms.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| **Cache Key** | A unique identifier derived from the input parameters used to locate a cached result. |
| **Cache Hit** | A scenario where the requested data is found in the cache and is valid for reuse. |
| **Cache Miss** | A scenario where the requested data is either absent from the cache or has expired. |
| **TTL (Time to Live)** | A policy defining the duration for which a cached result remains valid before being considered stale. |
| **Eviction** | The process of removing items from the cache to free up space, typically governed by a specific algorithm. |
| **Determinism** | The property of a process where the same input always produces the exact same output. |
| **Staleness** | The state where the cached data no longer reflects the current state of the source of truth. |

## Core Concepts
The fundamental principle of Result Caching is the trade-off between **data freshness** and **system performance**.

### Determinism and Idempotency
For a result to be safely cached, the underlying operation should ideally be deterministic. If a function relies on external state that changes frequently or unpredictably (e.g., `GetSystemTime()`), caching the result will lead to incorrect behavior.

### The Cache Key Derivation
The integrity of a result cache depends entirely on the uniqueness and consistency of the cache key. A key must encapsulate all variables that influence the output. If a function `CalculateTax(amount, region)` is cached only by `amount`, the system will return incorrect results for different regions.

> [!TIP]
> Think of a cache key as a mathematical mapping. If $f(x, y) = z$, then the key must be a unique representation of $(x, y)$. If the key only represents $(x)$, the mapping is broken.

### Cost-Benefit Analysis
Caching is not "free." It introduces memory overhead and architectural complexity. A result should only be cached if:
1. The cost of computation/retrieval > the cost of cache lookup + storage.
2. The data is requested more than once within its useful lifespan.

## Standard Model
The standard model for Result Caching follows a predictable request-response lifecycle:

1.  **Request Interception:** The system receives a request with specific input parameters.
2.  **Key Generation:** A unique cache key is generated based on the inputs.
3.  **Lookup:** The cache storage is queried using the key.
4.  **Decision Branch:**
    *   **Hit:** If the key exists and is valid, the cached result is returned immediately.
    *   **Miss:** If the key does not exist or is expired, the expensive operation is executed.
5.  **Population (on Miss):** The result of the operation is stored in the cache with the associated key and a TTL.
6.  **Delivery:** The result is returned to the requester.

## Common Patterns

### Cache-Aside (Lazy Loading)
The application is responsible for checking the cache. If the data is missing, the application fetches it from the source and manually updates the cache. This is the most common pattern for general-purpose application caching.

### Read-Through
The application treats the cache as the primary data store. If a miss occurs, the cache library/provider is responsible for fetching the data from the source and updating itself before returning the result to the application.

### Memoization
A specific form of result caching used at the function level within a single execution context. It stores the results of expensive function calls and returns the cached result when the same inputs occur again.

> [!IMPORTANT]
> Memoization is typically scoped to the lifetime of a process or a single request, whereas general result caching is often shared across multiple requests or nodes.

## Anti-Patterns

### Caching Non-Deterministic Results
Caching results that depend on volatile state (like a random number generator or a real-time clock) leads to "ghost" data that does not reflect reality.

### The "Cache Stampede" (Thundering Herd)
Occurs when a popular cached item expires and multiple concurrent requests all identify a cache miss simultaneously. This causes all requests to hit the source of truth at once, potentially crashing the underlying system.

### Over-Caching
Caching every possible result regardless of frequency. This leads to high memory consumption and "cache pollution," where rarely used data displaces frequently used data.

> [!CAUTION]
> Avoid tight coupling between the cache key format and the internal data structures of the source system. If the source schema changes but the cache key does not, the system may return corrupted or incompatible data.

## Edge Cases

### The "Zero" or "Null" Result
Deciding whether to cache empty results or error states is a critical edge case. Failing to cache a "Not Found" result can lead to a "Negative Cache Miss" where the system repeatedly queries the database for a non-existent record, causing unnecessary load.

### Time-of-Check to Time-of-Use (TOCTOU)
In highly concurrent systems, a result might be validated in the cache, but by the time it is delivered, the source of truth has changed. Systems requiring strict linearizability should approach result caching with extreme caution.

### Large Payload Overhead
If the result being cached is massive (e.g., a 50MB JSON blob), the overhead of serializing, de-serializing, and transferring the data from the cache store to the application may exceed the time saved by not querying the database.

## Related Topics
*   **105. Cache Invalidation Strategies:** Methods for purging stale data.
*   **202. Distributed Systems Consistency:** How caching affects CAP theorem trade-offs.
*   **308. Database Query Optimization:** Alternative methods to reduce retrieval costs.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |