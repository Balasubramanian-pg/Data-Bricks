# 103. Caching Patterns for BI

Canonical documentation for [103. Caching Patterns for BI](Phase 6/103. Caching Patterns for BI.md). This document defines concepts, terminology, and standard usage.

## Purpose
Business Intelligence (BI) systems often operate on massive datasets where direct, real-time computation of every query is computationally expensive or introduces unacceptable latency. Caching patterns for BI exist to bridge the performance gap between the raw data storage layer and the end-user interface. 

The primary objective of these patterns is to optimize the balance between data freshness, system throughput, and query response times. By strategically storing intermediate or final query results, organizations can reduce the load on underlying data warehouses and provide a seamless interactive experience for analysts and decision-makers.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on architectural logic rather than specific software configurations.

## Scope
This documentation covers the architectural strategies used to store and reuse data within the BI lifecycle.

> [!IMPORTANT]
> **In scope:**
> * Data persistence strategies at the semantic and visualization layers.
> * Cache invalidation logic and synchronization.
> * Performance optimization for aggregate and granular data retrieval.
> * Theoretical boundaries of data freshness vs. latency.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Power BI Import mode, Tableau Hyper, Snowflake Result Cache).
> * Hardware-level CPU/L1/L2 caching.
> * Network-level Content Delivery Networks (CDNs) for static assets.

## Definitions
| Term | Definition |
|------|------------|
| Cache Hit Ratio | The percentage of queries served from the cache versus those requiring a trip to the source system. |
| TTL (Time-to-Live) | The duration for which a cached object is considered valid before it must be refreshed or evicted. |
| Cache Invalidation | The process of declaring cached data as "stale" or "dirty" based on time or event-driven triggers. |
| Semantic Layer | An abstraction layer that maps complex data structures to business terms, often serving as the primary host for caching logic. |
| Pre-aggregation | The process of pre-calculating and storing summarized data (e.g., monthly totals) to avoid on-the-fly computation. |
| Cold Start | The performance lag experienced when a cache is empty and the system must populate it from the source. |
| Warm-up | The proactive process of executing common queries to populate the cache before users request them. |

## Core Concepts
The fundamental challenge in BI caching is the **Consistency-Latency Trade-off**. As data moves from the source of truth to the user's screen, each caching layer increases performance but introduces a risk of data staleness.

### The Freshness Gradient
Caching is not a binary state but a gradient. High-frequency operational reporting requires low TTLs, while strategic, long-term trend analysis can tolerate higher TTLs.

### Eviction Policies
Since cache storage is finite, systems must decide which data to discard. Common policies include:
* **Least Recently Used (LRU):** Discards the data that hasn't been accessed for the longest time.
* **Least Frequently Used (LFU):** Discards data with the lowest access count.
* **Time-Based:** Discards data strictly based on the expiration of its TTL.

> [!TIP]
> Think of a BI cache like a library's "Reserved" shelf. The most popular books (common queries) are kept behind the desk for instant access, while less common requests require a trip to the deep stacks (the data warehouse).

## Standard Model
The standard model for BI caching follows a multi-tiered approach, where each tier serves a specific purpose in the data delivery pipeline:

1.  **Source/Warehouse Layer:** The system of record. Caching here is usually temporary and focused on identical query strings.
2.  **Semantic/Aggregation Layer:** The "Brain" of the BI system. This layer caches business logic results, joins, and calculated measures.
3.  **Presentation/Client Layer:** The user's browser or application. This layer caches the visual representation of the data (e.g., the rendered chart).

## Common Patterns

### 1. Result Set Caching
The simplest form of caching where the exact output of a specific query is stored. If a user requests the same report with the exact same filters, the system returns the stored result.

### 2. Pre-calculated Aggregates (MOLAP Pattern)
Instead of caching specific query results, the system pre-calculates all possible combinations of dimensions and measures (a "cube"). This allows for high-speed "slicing and dicing" without re-querying the source.

### 3. Fragment Caching
The system breaks a complex dashboard into individual components (fragments). If only one chart in a dashboard requires fresh data, only that fragment is re-queried, while the rest are served from the cache.

### 4. Lazy Loading / Just-in-Time (JIT) Caching
Data is only cached once a user requests it. This minimizes storage costs but results in a "Cold Start" penalty for the first user who accesses the data after a refresh.

### 5. Proactive Caching (Cache Warming)
The system identifies "high-traffic" reports and executes them on a schedule (e.g., every morning at 6:00 AM) so the cache is "warm" before the business day begins.

## Anti-Patterns

### The "Cache Everything" Fallacy
Attempting to cache every possible permutation of a high-cardinality dataset. This leads to "cache bloat," where the overhead of managing the cache exceeds the performance gains.

### Manual Invalidation Dependency
Relying on human intervention to clear caches when data changes. This inevitably leads to "Data Drift," where different users see different versions of the truth.

### Ignoring Row-Level Security (RLS)
Caching a generic version of a report that contains sensitive data. If the cache is not "user-aware," User A might see data intended only for User B.

> [!CAUTION]
> Never implement a shared cache for reports involving Row-Level Security unless the cache key includes the user's unique security identity.

## Edge Cases

### Non-Deterministic Functions
Queries involving functions like `GETDATE()` or `RANDOM()` are difficult to cache because the "correct" result changes every second, even if the underlying data does not.

### High-Cardinality Filters
When users have thousands of filter options (e.g., searching by a specific Customer ID), the likelihood of a "Cache Hit" drops significantly, making result-set caching ineffective.

### Write-Back Scenarios
In BI tools that allow users to input data (e.g., budgeting or forecasting), the cache must be invalidated immediately upon the "write" action to ensure the user sees their changes reflected in the "read" view.

## Related Topics
* **Semantic Modeling:** The structure of data that informs how it can be cached.
* **Data Orchestration:** The scheduling of data loads which triggers cache invalidation.
* **Query Optimization:** The process of making the underlying queries faster, reducing the reliance on caching.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |