# 105. Disk Caching

Canonical documentation for [105. Disk Caching](Phase 6/105. Disk Caching.md). This document defines concepts, terminology, and standard usage.

## Purpose
Disk caching exists to bridge the performance gap between high-speed volatile memory (RAM) and relatively slow non-volatile storage media (HDDs, SSDs, or NVMe drives). Because physical disk I/O is several orders of magnitude slower than CPU processing and memory access, disk caching minimizes the frequency of physical disk operations by keeping frequently or recently accessed data in a faster intermediate storage layer.

The primary objectives of disk caching are to reduce latency, increase throughput, and minimize wear on physical storage components.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural principles of caching rather than specific operating system kernels or hardware controller firmware.

## Scope
This documentation covers the logical and physical mechanisms used to cache data between an application and the physical storage medium.

> [!IMPORTANT]
> **In scope:**
> * Operating system page caches and buffer caches.
> * Hardware-level controller caches (on-disk or RAID controller memory).
> * Read and write strategies (Write-back vs. Write-through).
> * Cache eviction algorithms and data integrity principles.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Linux `kswapd` specifics, Windows `Cache Manager` internals).
> * Application-level caching (e.g., Redis, Memcached).
> * Web browser caching or Content Delivery Networks (CDNs).

## Definitions
| Term | Definition |
|------|------------|
| Page Cache | A transparent cache of disk-backed pages kept in RAM by the operating system. |
| Dirty Page | A memory page that has been modified in the cache but has not yet been written back to the physical disk. |
| Flush / Sync | The process of writing dirty pages from the cache to the non-volatile storage medium. |
| IOPS | Input/Output Operations Per Second; a standard performance metric for storage devices. |
| Latency | The time delay between a request for data and the start of the data transfer. |
| Locality of Reference | The tendency of a processor to access the same set of memory locations repetitively over a short period. |
| Thrashing | A state where the system spends more time moving data in and out of the cache than performing actual work. |

## Core Concepts
Disk caching relies on the principle of **Locality of Reference**, which suggests that data accessed recently is likely to be accessed again soon (Temporal Locality) and that data located near recently accessed data is likely to be accessed soon (Spatial Locality).

### The Hierarchy of Storage
Caching operates on the principle that smaller, faster, and more expensive memory can act as a front-end for larger, slower, and cheaper storage. In the context of disk caching, the "fast" layer is typically System RAM or dedicated controller RAM, while the "slow" layer is the persistent disk.

> [!TIP]
> Think of disk caching like a librarian's desk. The "Disk" is the massive archive in the basement. The "Cache" is the small stack of books the librarian keeps on their desk. If a patron asks for a book already on the desk, the request is fulfilled instantly. If not, the librarian must make the slow trip to the basement.

### Volatility and Persistence
The most critical trade-off in disk caching is between performance and data safety. Because most caches reside in volatile RAM, data stored in the cache but not yet written to disk is at risk of loss during a power failure or system crash.

## Standard Model
The standard model for disk caching involves an intermediary layer that intercepts all I/O requests between the File System and the Storage Driver.

1.  **Read Request:** The system checks if the requested data block is in the cache (a "Cache Hit"). If present, data is returned immediately. If not (a "Cache Miss"), the data is read from the disk, stored in the cache, and then returned to the requester.
2.  **Write Request:** The system determines whether to write the data immediately to the disk or to mark the cache page as "dirty" and return a success message to the application immediately, deferring the physical write.
3.  **Eviction:** When the cache is full, the system must choose which data to remove to make room for new data, typically using an algorithm like Least Recently Used (LRU).

## Common Patterns

### Write-Through Caching
In a write-through pattern, data is written to the cache and the physical disk simultaneously. The write is only considered complete when the disk confirms the data is persisted.
*   **Benefit:** High data integrity; the cache and disk are always synchronized.
*   **Drawback:** Write performance is limited by the speed of the underlying disk.

### Write-Back (Write-Behind) Caching
In a write-back pattern, data is written only to the cache. The application receives a "success" signal immediately. The system flushes the dirty pages to the disk at a later time or under specific conditions (e.g., cache pressure or a timer).
*   **Benefit:** Significantly higher write performance and reduced disk contention.
*   **Drawback:** Risk of data loss if the system fails before the flush occurs.

### Read-Ahead (Prefetching)
The system predicts that the application will soon request the data blocks immediately following the current request. It proactively loads these blocks into the cache.
*   **Benefit:** Greatly improves performance for sequential workloads (e.g., video playback or large file copies).

## Anti-Patterns

### Double Caching
Occurs when an application (like a database) maintains its own internal cache of data while the operating system also maintains a page cache of the same data.
*   **Consequence:** Wasted RAM and unnecessary CPU overhead for memory management.

### Bypassing Cache for Small Random I/O
Using "Direct I/O" (bypassing the cache) for small, frequent, random read/write operations.
*   **Consequence:** Severe performance degradation as every operation incurs the full latency of the physical hardware.

> [!CAUTION]
> Disabling write-back caching on systems without a Battery Backed Unit (BBU) or Uninterruptible Power Supply (UPS) can lead to catastrophic filesystem corruption during power loss.

## Edge Cases

### Cache Thrashing
When the "working set" of data (the data actively being used) is larger than the available cache size, the system constantly evicts and reloads data. This can lead to a situation where the system performs worse than if there were no cache at all, due to the overhead of cache management.

### Cold Start
Immediately after a system reboot, the cache is empty (cold). Performance will be significantly lower until the cache is "warmed" with frequently accessed data.

### Large Sequential Writes
In some scenarios, a massive sequential write (e.g., a multi-terabyte backup) can "pollute" the cache, evicting useful small-file data that the system needs for general responsiveness. Modern kernels often implement "streaming" logic to prevent this.

## Related Topics
*   **Virtual Memory:** The management of memory that allows the cache to expand and contract based on system demand.
*   **Journaling Filesystems:** Mechanisms that work alongside disk caching to ensure metadata consistency.
*   **RAID Controllers:** Hardware devices that often include dedicated non-volatile cache memory.
*   **Solid State Drives (SSD):** Media that often use internal DRAM or SLC-caches to mask the latency of NAND flash.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |