# 125. Data Quality SLAs

Canonical documentation for [125. Data Quality SLAs](Phase 6/125. Data Quality SLAs.md). This document defines concepts, terminology, and standard usage.

## Purpose
Data Quality Service Level Agreements (SLAs) exist to formalize the expectations between data producers and data consumers regarding the reliability, health, and fitness-for-use of data assets. In modern data ecosystems, data is often treated as a product; SLAs provide the necessary framework to ensure that this product meets specific, measurable standards.

The primary problem space addressed by Data Quality SLAs is the "trust gap" in data engineering. Without formal agreements, consumers (such as data scientists, analysts, or automated downstream systems) operate under assumptions of data integrity that may not align with the producer's operational realities. SLAs provide a mechanism for accountability, incident prioritization, and resource allocation.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative. It focuses on the governance and structural requirements of SLAs rather than specific software configurations.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * The hierarchical relationship between SLIs, SLOs, and SLAs.
> * Standard dimensions of data quality used for measurement.
> * Governance frameworks for defining and enforcing data contracts.
> * Remediation and communication protocols for SLA breaches.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., dbt tests, Great Expectations, Monte Carlo).
> * Infrastructure-level SLAs (e.g., database uptime, network latency), unless they directly impact data quality metrics.
> * Code-level unit testing or CI/CD pipeline validation.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Service Level Indicator (SLI) | A quantitative measure of a specific aspect of data quality (e.g., the percentage of null values in a primary key column). |
| Service Level Objective (SLO) | A target value or range of values for a service level that is measured by an SLI (e.g., "99.9% of records must have a non-null primary key"). |
| Service Level Agreement (SLA) | The formal agreement—often including legal or operational consequences—that binds the producer to meet the SLOs. |
| Data Quality Dimension | A category of data quality measurement, such as Accuracy, Completeness, Consistency, Timeliness, Validity, or Uniqueness. |
| Error Budget | The maximum amount of "data unreliability" allowed before the SLA is breached and development must stop to focus on fixes. |
| Data Contract | A technical and legal specification defining the schema, metadata, and SLAs for a data exchange between parties. |

## Core Concepts
The fundamental idea behind Data Quality SLAs is the transition from "best-effort" data delivery to "guaranteed-standard" data delivery.

### The SLI-SLO-SLA Hierarchy
1.  **SLI (The Metric):** "What are we measuring?" (e.g., Freshness).
2.  **SLO (The Target):** "What is the threshold?" (e.g., Data must be no more than 3 hours old).
3.  **SLA (The Agreement):** "What happens if we fail?" (e.g., On-call engineer is paged; downstream stakeholders are notified via status page).

### Data Quality Dimensions
To build an effective SLA, one must select the appropriate dimensions:
*   **Freshness/Timeliness:** Is the data available when needed?
*   **Completeness:** Are there missing records or attributes?
*   **Accuracy:** Does the data represent the real-world entities it describes?
*   **Consistency:** Does the data match across different systems or snapshots?
*   **Validity:** Does the data conform to the defined format or schema?

> [!TIP]
> Think of a Data Quality SLA like a nutrition label on food. The label (SLA) tells the consumer exactly what is inside (SLIs) and guarantees that the ingredients meet safety standards (SLOs). If the food is expired or contaminated (SLA breach), there is a clear process for a recall (Remediation).

## Standard Model
The standard model for Data Quality SLAs follows a lifecycle of definition, measurement, and enforcement:

1.  **Discovery:** Identify critical data elements (CDEs) that drive business value.
2.  **Definition:** Collaborate with stakeholders to set realistic SLOs based on business impact.
3.  **Instrumentation:** Implement monitors (SLIs) to track performance against SLOs.
4.  **Reporting:** Provide transparent dashboards showing current status and historical trends.
5.  **Governance:** Define the "consequences" of a breach, such as automated pipeline halts or mandatory post-mortems.

## Common Patterns
*   **Tiered SLAs:** Applying different levels of rigor based on data criticality (e.g., "Tier 1/Gold" for financial reporting with 99.9% SLOs, "Tier 3/Bronze" for experimental sandboxes with 90% SLOs).
*   **Freshness-First:** Prioritizing the arrival time of data as the primary SLI for real-time streaming applications.
*   **Schema Evolution SLAs:** Agreements that specify how much notice a producer must give before changing a data structure.

## Anti-Patterns
*   **The 100% Trap:** Setting SLOs at 100%. This is mathematically impossible to maintain in distributed systems and leads to "alert fatigue."
*   **Measuring Everything:** Creating SLIs for every column in a warehouse, which creates noise and obscures the metrics that actually matter for business decisions.
*   **Silent Failures:** Having an SLA but no automated notification system when it is breached.
*   **Producer-Only Definition:** Defining SLAs without consulting the data consumers, leading to metrics that don't reflect actual business needs.

> [!CAUTION]
> Avoid setting SLAs that are tighter than the upstream source's capabilities. If your source system only updates every 24 hours, promising a 1-hour freshness SLA to your consumers is a guaranteed failure.

## Edge Cases
*   **Late-Arriving Data:** In streaming architectures, data may arrive after the SLA window has closed. The SLA must define whether "Freshness" refers to the event time or the processing time.
*   **Third-Party Data:** When data is sourced from an external vendor, the internal SLA is often dependent on the vendor's performance. These are "Pass-through SLAs."
*   **Historical Restatements:** If data from six months ago is corrected, does that constitute an SLA breach for the current period? Canonical models usually treat "Restatements" as a separate category of quality agreement.
*   **Schema Drift:** When an upstream system adds a field that doesn't break the pipeline but violates "Validity" logic, the SLA must define if this is a "Warning" or a "Critical Breach."

## Related Topics
*   **Data Contracts:** The technical implementation of SLAs within a schema.
*   **Observability:** The tooling and practices required to measure SLIs.
*   **Data Governance:** The overarching framework that mandates the use of SLAs.
*   **Incident Management:** The process triggered when an SLA is breached.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |