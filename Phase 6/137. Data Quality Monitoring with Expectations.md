# 137. Data Quality Monitoring with Expectations

Canonical documentation for [137. Data Quality Monitoring with Expectations](Phase 6/137. Data Quality Monitoring with Expectations.md). This document defines concepts, terminology, and standard usage.

## Purpose
Data Quality Monitoring with Expectations addresses the critical need for trust and reliability in data ecosystems. In modern data engineering, data is often fluid, arriving from disparate sources with varying degrees of structure and reliability. Traditional software testing focuses on code logic, but data quality monitoring focuses on the state and shape of the data itself.

The "Expectations" framework provides a declarative way to express assertions about data. By defining what the data *should* look like, organizations can detect "silent failures"â€”scenarios where data pipelines complete successfully but produce incorrect, incomplete, or malformed data that compromises downstream analytics, machine learning models, and business decisions.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural pattern rather than specific software libraries.

## Scope
The scope of this topic covers the methodology of defining, applying, and monitoring declarative data assertions throughout the data lifecycle.

> [!IMPORTANT]
> **In scope:**
> * Declarative definition of data constraints (Expectations).
> * Validation logic and execution patterns.
> * Metadata collection and reporting of validation results.
> * Integration of expectations into Data Contracts.
> * Theoretical boundaries of data observability.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations or proprietary DSL (Domain Specific Language) syntaxes.
> * Physical storage layer optimization.
> * General network or infrastructure monitoring.

## Definitions
| Term | Definition |
|------|------------|
| Expectation | A declarative assertion or rule regarding a specific characteristic of a dataset (e.g., "column X must not be null"). |
| Validation | The process of evaluating a dataset against a set of Expectations to determine compliance. |
| Expectation Suite | A logical grouping of multiple Expectations applied to a specific data asset or pipeline stage. |
| Data Drift | A shift in the statistical properties or distribution of data over time that may invalidate previous assumptions. |
| Schema Drift | Unannounced or unexpected changes in the structure (columns, types, nesting) of a dataset. |
| Profiling | The automated process of analyzing a dataset to generate candidate Expectations based on observed statistics. |
| Data Contract | A formal agreement between a data provider and a consumer that includes Expectations as a core component of the interface. |

## Core Concepts
The fundamental idea behind Expectations is the shift from **imperative validation** (writing custom scripts to check data) to **declarative validation** (stating the desired state).

### Declarative Assertions
Expectations are expressed as "What" rather than "How." Instead of writing a SQL query to find nulls, a user defines an expectation: `expect_column_values_to_not_be_null`. This allows the underlying engine to optimize the check across different compute environments (Spark, SQL, Pandas).

### Unit Testing for Data
Expectations function as unit tests for your data. Just as software engineers write tests to ensure code behaves as expected, data engineers use Expectations to ensure data arrives and transforms as expected.

> [!TIP]
> Think of Expectations as a "Living Documentation" of your data. A well-defined suite of expectations tells a new user exactly what the data represents and what constraints it must satisfy without them having to query the raw data.

### The Validation Lifecycle
1.  **Definition:** Expectations are authored manually or generated via profiling.
2.  **Execution:** The validation engine runs the expectations against a batch of data.
3.  **Result Generation:** A validation result object is produced, containing success/failure status and observed metrics.
4.  **Action:** Based on the result, the system triggers alerts, halts pipelines, or quarantines data.

## Standard Model
The standard model for Data Quality Monitoring with Expectations follows a layered approach integrated into the data pipeline:

1.  **Input Validation (The Gatekeeper):** Expectations applied at the ingestion point to ensure raw data meets the minimum requirements before entering the warehouse or lake.
2.  **Transformation Validation (The Auditor):** Expectations applied after complex joins or aggregations to ensure business logic has not introduced anomalies (e.g., ensuring a join didn't result in a Cartesian product).
3.  **Output Validation (The Publisher):** Expectations applied to the final "Gold" or "Mart" layer to ensure data consumed by end-users or APIs is pristine.

> [!IMPORTANT]
> The standard model emphasizes **automated metadata capture**. Every validation run should log its results to a centralized store to enable long-term trend analysis of data quality.

## Common Patterns
*   **The Circuit Breaker:** If a critical Expectation fails (e.g., a primary key is null), the data pipeline is automatically halted to prevent "poisoning" downstream tables.
*   **The Warning Track:** For non-critical issues (e.g., a slight shift in mean value), the pipeline continues, but an alert is sent to data stewards for investigation.
*   **Golden Store Comparison:** Validating current data against a "Golden" reference dataset to ensure consistency in categorical values or reference codes.
*   **Cross-Table Validation:** Expectations that assert consistency between two different datasets (e.g., the sum of transactions in the `Orders` table must match the sum in the `Ledger` table).

## Anti-Patterns
*   **Alert Fatigue:** Defining too many granular expectations that trigger frequent, low-priority notifications, leading teams to ignore all alerts.
*   **Hard-coding Expectations in ETL:** Embedding validation logic directly inside transformation code, making it difficult to update rules without redeploying the entire pipeline.
*   **Testing in Production Only:** Failing to run Expectations in staging or CI/CD environments, allowing schema changes to break pipelines only after they reach production.
*   **Static Thresholds for Dynamic Data:** Using fixed numeric ranges for data that naturally grows or shifts over time, leading to false positives.

> [!CAUTION]
> Avoid circular dependencies where an Expectation relies on a value that is calculated by the very pipeline it is supposed to validate.

## Edge Cases
*   **Late-Arriving Data:** Expectations may fail if they assume a complete daily batch, but data arrives in fragments. Validation logic must be "window-aware."
*   **Empty Datasets:** A pipeline that returns zero rows might pass "null checks" but fail the business requirement of having data. Expectations should include row-count minimums.
*   **Schema Evolution:** When a source system adds a column, Expectations must be flexible enough to allow the new column while still enforcing constraints on existing ones.
*   **High-Cardinality Shifts:** In categorical data (like UUIDs), traditional statistical expectations may fail to capture quality issues. These require specialized uniqueness or format-based expectations.

## Related Topics
*   **Data Observability:** The broader practice of monitoring the health of the entire data stack.
*   **Data Lineage:** Understanding where data comes from to identify the root cause of an Expectation failure.
*   **Data Governance:** The policy framework that dictates which Expectations are required for compliance and security.
*   **Data Contracts:** The formalization of Expectations into a shared agreement between producers and consumers.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |