# 107. CPU Tuning for Spark

Canonical documentation for [107. CPU Tuning for Spark](Phase 6/107. CPU Tuning for Spark.md). This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of CPU tuning in Apache Spark is to optimize the allocation and utilization of processing resources to maximize throughput and minimize job latency. In a distributed computing environment, the CPU is the primary engine for data transformation, serialization, and computation. Improper CPU configuration leads to resource contention, excessive context switching, or under-utilization of hardware, all of which degrade performance and increase operational costs.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the Spark engine's relationship with logical and physical processing units.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * Relationship between executors, cores, and task slots.
> * Theoretical limits of parallelism and concurrency.
> * Impact of CPU allocation on memory management and I/O.
> * Strategies for balancing task distribution across available processing units.

> [!WARNING]
> **Out of scope:**
> * Specific hardware architecture optimizations (e.g., AVX-512, ARM vs. x86).
> * Vendor-specific managed service auto-scaling algorithms.
> * Operating system-level kernel tuning (e.g., sysctl, cgroups).

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Executor Core | A logical thread of execution available to a Spark Executor. It represents a single "slot" for a task. |
| Task Slot | The logical unit within an executor that can run exactly one Spark task at a time. |
| Parallelism | The number of tasks that can be executed simultaneously across the entire cluster. |
| Over-subscription | The practice of assigning more tasks to a node than there are physical CPU cores available. |
| Context Switching | The process of a CPU switching from one task/thread to another, incurring computational overhead. |
| Data Skew | A condition where data is unevenly distributed across partitions, causing some CPU cores to work longer than others. |

## Core Concepts
Explain the fundamental ideas.

### The Task-to-Core Mapping
Spark operates on a simple fundamental principle: one task occupies one executor core (slot) at any given time. The total number of available slots in the cluster is the product of the number of executors and the cores assigned per executor.

### CPU-Bound vs. I/O-Bound Workloads
*   **CPU-Bound:** Workloads involving heavy computation (e.g., complex mathematical functions, encryption, or deep nested parsing). These require high-performance cores and benefit from a 1:1 ratio of tasks to physical cores.
*   **I/O-Bound:** Workloads involving significant data movement (e.g., reading from remote storage or shuffling over the network). These often benefit from slight over-subscription to keep the CPU busy while waiting for data.

> [!TIP]
> Think of CPU cores as "chefs" in a kitchen. If you have five chefs but only one stove (I/O), four chefs will stand idle. If you have one chef and five stoves, the chef will be overwhelmed by context switching between pots. Tuning is the art of matching the number of chefs to the available equipment and the complexity of the recipe.

## Standard Model
The generally accepted model for CPU tuning in Spark revolves around the "Balanced Executor" approach.

1.  **Cores per Executor:** The industry standard is to assign **4 to 6 cores per executor**. 
    *   Fewer than 5 cores can lead to poor HDFS throughput (due to limited concurrent threads for I/O).
    *   More than 6 cores can lead to excessive Garbage Collection (GC) pressure, as more tasks share the same heap memory, increasing the frequency of "Stop-the-World" events.
2.  **Parallelism Ratio:** The number of partitions (and thus tasks) should typically be **2 to 3 times** the total number of available cores in the cluster. This allows for "tail-end" tasks to fill in gaps when some tasks finish faster than others, maintaining high utilization.
3.  **Cluster Utilization:** Total Cores = (Number of Nodes * Cores per Node) - (Resources reserved for OS and Cluster Manager).

## Common Patterns
*   **Static Allocation:** Manually defining `spark.executor.cores` and `spark.executor.instances` based on known hardware limits. This is used for predictable, recurring production jobs.
*   **Dynamic Allocation:** Allowing Spark to request and release executors based on the backlog of pending tasks. CPU tuning here focuses on setting the upper and lower bounds of cores to prevent cluster exhaustion.
*   **High-Concurrency Clusters:** In shared environments, limiting cores per executor to 2 or 3 to allow more executors to coexist, improving multi-tenant fairness at the cost of some inter-executor communication overhead.

## Anti-Patterns
Common mistakes or discouraged practices.

*   **The "Fat" Executor:** Assigning all available cores on a physical node to a single Spark executor (e.g., 32 or 64 cores). This leads to massive GC overhead and potential memory management bottlenecks.
*   **The "Tiny" Executor:** Assigning only 1 core per executor. This eliminates the benefits of shared memory within an executor (like broadcast variables) and increases the overhead of managing many small JVMs.
*   **Ignoring Virtualization:** Treating "vCPUs" in cloud environments as equivalent to physical cores without accounting for hyper-threading or underlying hardware over-provisioning.

> [!CAUTION]
> Avoid setting `spark.default.parallelism` or `spark.sql.shuffle.partitions` to a value lower than the total number of cores in your cluster. This results in idle CPUs and wasted resources.

## Edge Cases
*   **Python UDFs (PySpark):** When using non-vectorized Python User Defined Functions, Spark spawns a separate Python process for each task. This can lead to CPU contention between the JVM and the Python interpreter on the same node, effectively doubling the CPU demand per task.
*   **Extreme Data Skew:** If one partition is significantly larger than others, a single CPU core will remain active while all other cores in the cluster sit idle, waiting for the stage to complete. CPU tuning cannot fix skew; data repartitioning is required.
*   **Broadcast Joins:** While primarily a memory concern, the serialization and deserialization of broadcasted data are CPU-intensive. Large broadcasts can cause CPU spikes across all executors simultaneously.

## Related Topics
*   **102. Memory Management in Spark:** The relationship between core count and heap pressure.
*   **105. Data Partitioning Strategies:** How partition counts drive CPU utilization.
*   **201. Garbage Collection Tuning:** Managing the impact of multi-threaded execution on the JVM.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |