# 132. Spark UI Advanced Analysis

Canonical documentation for [132. Spark UI Advanced Analysis](Phase 6/132. Spark UI Advanced Analysis.md). This document defines concepts, terminology, and standard usage.

## Purpose
The Spark User Interface (UI) serves as the primary diagnostic window into the execution of distributed computing workloads. Advanced analysis of the Spark UI is required to move beyond simple job status monitoring and into the realm of performance tuning, resource optimization, and root-cause analysis of application failures. This topic addresses the systematic interpretation of complex metrics to resolve bottlenecks such as data skew, memory pressure, and inefficient scheduling.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the logical structure of the Spark UI rather than specific vendor-wrapped versions.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * Interpretation of the Directed Acyclic Graph (DAG) and Event Timeline.
> * Analysis of Task-level metrics (Duration, GC Time, Shuffle Read/Write).
> * Identification of performance bottlenecks (Skew, Spill, Locality).
> * Correlation between SQL execution plans and physical execution stages.

> [!WARNING]
> **Out of scope:**
> * External monitoring tools (e.g., Prometheus, Grafana, Ganglia).
> * Operating system-level kernel debugging.
> * Specific cloud vendor UI extensions or proprietary monitoring wrappers.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| **DAG Visualization** | A visual representation of the sequence of RDD dependencies and the stages they form. |
| **Data Skew** | A condition where a small number of partitions hold significantly more data than others, leading to "straggler" tasks. |
| **Disk Spill** | The process of moving data from RAM to disk when the execution or storage memory exceeds the allocated fraction. |
| **Locality Wait** | The time a task spends waiting to launch on a node that contains its preferred data. |
| **Scheduler Delay** | The time elapsed between task submission and the actual start of execution on an executor. |
| **Straggler** | A task that takes significantly longer to complete than the median task duration within the same stage. |

## Core Concepts
Advanced analysis relies on understanding the hierarchy of execution and the flow of data across the cluster.

### The Execution Hierarchy
Spark organizes work into a strict hierarchy: **Job > Stage > Task**. 
* **Jobs** are triggered by actions.
* **Stages** are defined by "shuffle" boundaries (wide dependencies).
* **Tasks** are the smallest unit of work, executed on a single partition.

### Quantile Analysis
The Summary Metrics table in the Stage detail view provides the 0th (Min), 25th, 50th (Median), 75th, and Max percentiles for task metrics. Advanced analysis focuses on the delta between the 75th percentile and the Max. A large gap here is the definitive signature of data skew or hardware-specific issues.

> [!TIP]
> Think of the Spark UI as a medical imaging tool. The Jobs tab is the general physical exam, the Stages tab is the X-ray, and the Task metrics are the microscopic biopsy.

## Standard Model
The standard model for advanced analysis follows a "Top-Down Diagnostic Flow":

1.  **Identify the Bottleneck Job:** Locate the job with the highest duration in the Jobs tab.
2.  **Locate the Critical Stage:** Within that job, identify the stage responsible for the majority of the execution time.
3.  **Analyze Task Distribution:** Examine the Summary Metrics for that stage. Compare "Duration" and "Shuffle Read Size" across quantiles.
4.  **Correlate with SQL Plan:** Use the SQL tab to map the stage back to the high-level code (e.g., a specific Join or Window function).
5.  **Check Resource Health:** Review the Executors tab to ensure no single executor is disproportionately burdened with GC (Garbage Collection) time or failed tasks.

## Common Patterns
Recurring patterns identified through advanced analysis:

*   **The Skew Pattern:** The Max task duration is 10x or more than the Median, and the "Shuffle Read Size" shows a similar disproportionate distribution.
*   **The Memory Pressure Pattern:** High "Spill (Memory)" and "Spill (Disk)" values in the stage summary. This indicates that the `spark.memory.fraction` is insufficient for the volume of data being processed in that stage.
*   **The Serialization Bottleneck:** High "Result Serialization Time" or "Task Deserialization Time" relative to the total duration, suggesting that the objects being passed are too large or the serializer is inefficient.
*   **The Small File Syndrome:** A stage with thousands of tasks, each processing only a few kilobytes of data, leading to high "Scheduler Delay" and "Task Deserialization Time."

## Anti-Patterns
Common mistakes or discouraged practices in Spark UI analysis.

*   **Average-Based Tuning:** Relying on the "Average" metric instead of quantiles. Averages hide stragglers and mask skew.
*   **Ignoring GC Time:** Failing to check the "GC Time" column in the Task table. High GC time often indicates that the executor heap is nearly full, which can lead to "Executor Lost" errors.
*   **Misinterpreting Shuffle:** Assuming all shuffle is bad. Shuffle is necessary for many operations; the goal is to optimize the *volume* and *distribution* of the shuffle, not necessarily to eliminate it at the cost of logic.

> [!CAUTION]
> Do not assume that a failed job's root cause is in the final stage. Often, the final stage fails due to an upstream data corruption or resource exhaustion that occurred several stages prior.

## Edge Cases
*   **Zombie Tasks:** In cases of speculative execution, the UI may show multiple attempts for the same task. Only the successful attempt's metrics contribute to the final result, but the "zombie" tasks still consume cluster resources.
*   **UI Truncation:** For jobs with millions of tasks, the Spark UI may truncate the task list to prevent the Driver from crashing due to out-of-memory errors. In these cases, the Summary Metrics remain accurate, but individual task details may be missing.
*   **Driver-Side Bottlenecks:** If the UI shows no active stages but the job is not finished, the bottleneck is likely on the Driver (e.g., `collect()`ing too much data or a complex broadcast variable generation).

## Related Topics
*   **101. Spark Core Architecture:** Understanding the Driver/Executor relationship.
*   **105. Memory Management:** Deep dive into execution vs. storage memory.
*   **112. Shuffle Service:** Mechanics of data movement between stages.
*   **140. Adaptive Query Execution (AQE):** How Spark attempts to fix skew and join types dynamically.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |