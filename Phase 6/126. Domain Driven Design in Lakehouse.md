# 126. Domain Driven Design in Lakehouse

Canonical documentation for [126. Domain Driven Design in Lakehouse](Phase 6/126. Domain Driven Design in Lakehouse.md). This document defines concepts, terminology, and standard usage.

## Purpose
Domain-Driven Design (DDD) in the context of a Lakehouse architecture addresses the challenges of scaling data management in large organizations. Traditionally, data lakes and warehouses suffered from "monolithic" bottlenecks where a central team lacked the domain expertise to manage diverse data sets effectively. 

By applying DDD principles to the Lakehouse—a unified platform that combines the cost-effectiveness of a data lake with the performance and ACID guarantees of a data warehouse—organizations can decentralize data ownership. This approach ensures that data is structured, governed, and evolved according to the business logic of the specific domain that produces or consumes it, rather than being forced into a generic, centralized schema.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the architectural intersection of DDD and Lakehouse paradigms.

## Scope
This documentation covers the strategic and tactical application of DDD principles to data architecture within a Lakehouse environment.

> [!IMPORTANT]
> **In scope:**
> * Strategic design patterns (Bounded Contexts, Ubiquitous Language) applied to data.
> * Tactical design patterns (Aggregates, Entities) within Lakehouse tables.
> * The relationship between domain ownership and data product delivery.
> * Governance and interoperability standards in a decentralized Lakehouse.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Databricks, Snowflake, AWS Lake Formation).
> * General software engineering DDD (e.g., microservices implementation) unless directly impacting data storage.
> * Physical storage optimization techniques (e.g., Z-Ordering, partitioning) not driven by domain logic.

## Definitions
| Term | Definition |
|------|------------|
| Bounded Context | A logical boundary within which a particular data model is defined and applicable. |
| Ubiquitous Language | A common language shared by domain experts and data engineers to describe data entities and processes. |
| Data Product | A high-quality, ready-to-use dataset or analytical output owned by a specific domain. |
| Aggregate (Data) | A cluster of related data entities treated as a single unit for data consistency and lifecycle management. |
| Lakehouse | A data management system that implements data warehouse-like features (ACID, schema enforcement) on top of low-cost cloud storage. |
| Source-Aligned Domain | A domain context focused on capturing and exposing data as it is generated by operational systems. |
| Consumer-Aligned Domain | A domain context focused on transforming and aggregating data to meet specific analytical or business use cases. |

## Core Concepts

### Strategic Alignment
DDD in a Lakehouse begins with identifying **Bounded Contexts**. Instead of a single "Enterprise Data Model," the Lakehouse is partitioned into logical zones owned by business units (e.g., Finance, Marketing, Logistics). Each unit defines its own schema and logic, preventing the "God Schema" problem where a single change breaks the entire system.

### Ubiquitous Language in Metadata
The Lakehouse metadata layer (catalog) must reflect the business language. Table names, column descriptions, and metric definitions should match the terminology used by domain experts. This reduces the "translation tax" between business requirements and technical implementation.

> [!TIP]
> Think of a Bounded Context as a "sovereign state" within the Lakehouse. While it must follow certain "international laws" (global governance), it has complete autonomy over its internal "laws" (business logic and schema).

### Data as a Product
In a DDD-oriented Lakehouse, data is not a byproduct of an application; it is a product. This means the domain team is responsible for the quality, uptime, and documentation of their data tables, just as a software team is responsible for an API.

## Standard Model

The standard model for DDD in a Lakehouse typically follows a decentralized version of the Medallion Architecture, mapped to domain boundaries:

1.  **Raw/Bronze (Domain-Owned):** Data is ingested from source systems into a domain-specific landing zone. It remains in its native format but is logically owned by the domain.
2.  **Cleaned/Silver (Domain-Owned):** The domain applies its **Ubiquitous Language**. Data is cleansed, and **Aggregates** are formed. This layer represents the "Source of Truth" for the domain's entities.
3.  **Curated/Gold (Domain or Consumer-Owned):** Data is transformed into **Data Products**. These may be specific to the domain or created by a consumer-aligned domain that joins data from multiple upstream bounded contexts.

> [!IMPORTANT]
> Global Governance (the "Mesh") provides the "interoperability standards" (e.g., identity management, common join keys like `customer_id`) that allow these independent domains to connect.

## Common Patterns

### The Aggregate Table Pattern
Instead of highly normalized tables, Lakehouse tables are often designed around DDD Aggregates. A "Sales Order" aggregate might include the header and line items in a single nested structure or a tightly coupled set of tables, ensuring that the entire business object is updated atomically.

### Ports and Adapters for Data
Domains expose data through "Output Ports" (standardized views or tables). If an internal domain model changes, the domain team provides an "Adapter" (a view or transformation) to ensure the public Data Product remains stable for downstream consumers.

### Shared Kernel
When two domains share a significant amount of logic or data (e.g., a shared definition of "Product"), they may manage a "Shared Kernel"—a set of tables or schemas that both domains collaborate on and agree to change only in tandem.

## Anti-Patterns

### The Data Swamp Monolith
Storing all data in a single "General" folder or schema without clear domain ownership. This leads to "tragedy of the commons" where no one is responsible for data quality.

### Technical-Only Partitioning
Partitioning data by technical metadata (e.g., `ingestion_timestamp` or `source_system_id`) rather than business-relevant domain boundaries.

### The "Anemic" Data Model
Creating tables that are merely "dumps" of database rows without applying business logic or domain context, forcing every consumer to reinvent the business logic.

> [!CAUTION]
> Avoid "Circular Domain Dependencies" where Domain A requires data from Domain B, which in turn requires data from Domain A to process its own raw inputs. This indicates poorly defined Bounded Contexts.

## Edge Cases

### Cross-Domain Joins
When a business question requires data from two disconnected Bounded Contexts (e.g., "Marketing Spend" vs. "Actual Revenue"), a new **Consumer-Aligned Domain** should be created to manage the join logic, rather than forcing one domain to take ownership of the other's logic.

### Legacy Monolith Ingestion
When ingesting from a legacy ERP that lacks domain boundaries, the Lakehouse must implement an **Anti-Corruption Layer (ACL)**. This is a transformation step that maps the messy legacy data into a clean, domain-oriented model before it reaches the Silver layer.

### Global Reference Data
Data like "Countries" or "Currencies" often doesn't fit neatly into a single business domain. These are typically handled by a "Master Data" domain that provides reference products to all other Bounded Contexts.

## Related Topics
*   **Data Mesh:** The organizational and architectural framework that often utilizes DDD in a Lakehouse.
*   **Medallion Architecture:** The data layering strategy commonly used to implement DDD stages.
*   **Data Governance:** The set of rules that ensure decentralized domains remain interoperable.
*   **Schema Evolution:** The technical process of managing changes within a Bounded Context.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |