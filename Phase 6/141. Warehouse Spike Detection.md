# 141. Warehouse Spike Detection

Canonical documentation for [141. Warehouse Spike Detection](Phase 6/141. Warehouse Spike Detection.md). This document defines concepts, terminology, and standard usage.

## Purpose
Warehouse Spike Detection exists to identify, quantify, and alert on anomalous surges in activity within a data warehouse environment. These surges typically manifest as sudden increases in compute resource consumption, storage utilization, or data ingestion volume. 

The primary objective of this topic is to provide a framework for distinguishing between expected growth (scalability) and unexpected anomalies (spikes) that could lead to budget overruns, performance degradation, or system instability. By establishing a standardized approach to detection, organizations can implement proactive governance and automated mitigation strategies.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the logic and methodology of detection rather than specific cloud provider tooling.

## Scope
Clarify what is in scope and out of scope for this topic.

> [!IMPORTANT]
> **In scope:**
> * Statistical methodologies for identifying anomalies in time-series warehouse data.
> * Definition of baselines and thresholding logic.
> * Classification of spike types (Compute, Storage, Ingress).
> * Governance frameworks for responding to detected spikes.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Snowflake Resource Monitors, BigQuery Quotas).
> * General performance tuning or query optimization.
> * Network-level DDoS detection (unless manifesting as warehouse compute spikes).

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Spike | A statistically significant deviation from the established baseline of warehouse activity over a specific temporal window. |
| Baseline | The historical average or expected range of activity, accounting for cyclical patterns and known growth trends. |
| Threshold | The specific value or percentage increase that triggers a detection event. |
| False Positive | A detected spike that is actually legitimate business activity (e.g., a planned end-of-quarter report). |
| Seasonality | Recurring fluctuations in warehouse activity that follow a predictable pattern (hourly, weekly, or monthly). |
| Compute Credit | A unit of measure for the consumption of processing power within a cloud data warehouse. |

## Core Concepts
Explain the fundamental ideas.

Warehouse Spike Detection relies on the principle of **Observability**. To detect a spike, the system must have continuous visibility into metadata regarding resource consumption.

**1. Time-Series Analysis**
Spike detection is fundamentally a time-series problem. Data points (credits used, rows loaded) are mapped against time. Detection algorithms look for "shocks" to the system where the current value exceeds the predicted value by a defined margin.

**2. Statistical Significance**
Not every increase is a spike. Detection models often use Z-scores (standard deviations from the mean) to determine if a surge is statistically significant. A common threshold is 2 or 3 standard deviations from the moving average.

> [!TIP]
> Think of Warehouse Spike Detection like a heart rate monitor: a sudden jump while resting is a cause for alarm (anomalous spike), but a jump while running is expected (planned workload). The system must know the "activity state" to be effective.

## Standard Model
The standard model for Warehouse Spike Detection follows a four-stage lifecycle:

1.  **Ingestion & Aggregation:** Collecting metadata from warehouse logs (e.g., query history, billing logs) and aggregating them into granular time buckets (e.g., 5-minute or 1-hour intervals).
2.  **Baseline Calculation:** Applying a rolling window (e.g., the last 30 days) to calculate the "normal" behavior. This must account for seasonality (e.g., Monday mornings are usually busier than Sunday nights).
3.  **Comparison & Evaluation:** Comparing the real-time or near-real-time consumption against the baseline. If the value exceeds the threshold, a "Spike Candidate" is identified.
4.  **Classification & Alerting:** Filtering the candidate against known schedules (planned ETL jobs). If the spike remains unexplained, an alert is dispatched to administrators.

## Common Patterns
Recurring patterns or approaches.

*   **Static Thresholding:** Setting a hard limit (e.g., "Alert if hourly spend exceeds $500"). This is simple but prone to false positives as the business grows.
*   **Dynamic/Adaptive Thresholding:** Using machine learning or moving averages to adjust the threshold based on recent history.
*   **Percentage-over-Baseline:** Identifying spikes based on relative growth (e.g., "Alert if current hour is 200% higher than the average of the previous four Mondays").
*   **Attribution-Based Detection:** Breaking down spikes by user, warehouse, or service account to identify the specific root cause immediately.

## Anti-Patterns
Common mistakes or discouraged practices.

*   **The "Boy Who Cried Wolf" Effect:** Setting thresholds too low, leading to alert fatigue where administrators ignore genuine spikes.
*   **Ignoring Cold Starts:** Failing to account for the initial surge when a warehouse wakes from a suspended state, which can look like a spike but is standard operational overhead.
*   **Reactive-Only Monitoring:** Only checking for spikes when the monthly bill arrives, rather than implementing real-time or daily detection.
*   **Global Thresholds:** Applying the same spike detection logic to a "Development" warehouse and a "Production" warehouse.

> [!CAUTION]
> Avoid tight coupling between detection alerts and automated "Kill Switches." Automatically terminating all queries during a spike can cause data corruption or critical business outages if the spike was actually a legitimate, high-priority process.

## Edge Cases
Explain unusual, ambiguous, or boundary scenarios.

*   **The "Slow Burn":** A gradual increase in consumption that never triggers a "spike" threshold but results in a massive month-end cost increase. This requires "Trend Detection" rather than "Spike Detection."
*   **Leap Years and Holidays:** Calendar anomalies can disrupt seasonality models. A spike on "Black Friday" may be anomalous compared to a standard Friday but expected for the retail industry.
*   **Bulk Historical Reloads:** When a system of record is re-synced, the warehouse may experience a massive, one-time ingestion spike. These should be flagged as "Expected Anomalies" in the detection system.
*   **First-Run Scenarios:** New warehouses lack historical data, making baseline calculation impossible for the first 7â€“30 days.

## Related Topics
*   **112. Cost Governance:** The broader framework of managing cloud spend.
*   **085. Resource Monitors:** The specific tools used to enforce limits.
*   **202. Workload Management:** The practice of prioritizing queries to prevent resource contention.
*   **Anomaly Detection (General):** The mathematical foundation for identifying outliers in data.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |