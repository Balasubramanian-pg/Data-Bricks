# 118. Certified Data Sets

Canonical documentation for [118. Certified Data Sets](Phase 6/118. Certified Data Sets.md). This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of Certified Data Sets is to establish a "single source of truth" within an organization’s data ecosystem. In modern data environments, proliferation of redundant, inconsistent, or low-quality data often leads to conflicting insights and eroded trust. Certified Data Sets address this by providing a curated layer of data that has been formally validated for accuracy, security, and compliance.

By identifying specific data assets as "Certified," an organization signals to data consumers—ranging from business analysts to machine learning models—that the data is reliable, maintained, and authoritative for decision-making.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the governance and structural requirements of data certification rather than specific software platforms.

## Scope
This documentation covers the theoretical framework, governance requirements, and operational lifecycle of data certification.

> [!IMPORTANT]
> **In scope:**
> * Criteria for data validation and endorsement.
> * Roles and responsibilities in the certification lifecycle.
> * Metadata and lineage requirements for certified assets.
> * Governance workflows for maintaining certification status.

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., Power BI Endorsement, Tableau Certification, Snowflake Data Clean Rooms).
> * Technical ETL/ELT code implementation details.
> * Specific industry-standard data models (e.g., FHIR, ISO 20022), though these may be used as criteria for certification.

## Definitions
| Term | Definition |
|------|------------|
| Data Certification | The formal process of verifying that a data set meets predefined standards for quality, security, and documentation. |
| Data Steward | A functional role responsible for the day-to-day management and quality of a specific data domain. |
| Data Lineage | The visual or technical map showing the origin, movement, and transformation of data over time. |
| Golden Record | The most accurate, complete, and authorized version of a data entity (e.g., "Customer"). |
| Metadata | Data that provides information about other data, such as ownership, last refresh date, and field descriptions. |
| Certification Decay | The process by which a certified data set loses its validity due to lack of maintenance or changing business logic. |

## Core Concepts
The fundamental idea behind Certified Data Sets is the transition from "Data Availability" to "Data Trust."

### The Trust Hierarchy
Data certification operates on the principle that not all data is created equal. While a "Raw" data set might be useful for exploratory data science, a "Certified" data set is required for financial reporting or regulatory compliance.

### The Seal of Approval
Certification acts as a visual and functional "seal of approval." It reduces the cognitive load on users by removing the need for them to perform their own validation of the source logic.

> [!TIP]
> Think of a Certified Data Set like "Certified Organic" produce. It doesn't mean other food is unusable, but the certification guarantees that specific, audited standards were followed during production, providing a higher level of confidence for the consumer.

### Ownership and Accountability
A data set cannot be certified without a designated owner. This ensures that when data quality issues arise, there is a clear path for remediation.

## Standard Model
The standard model for data certification follows a structured lifecycle:

1.  **Identification:** A data set is proposed for certification based on its high usage or criticality to business operations.
2.  **Profiling & Validation:** The data is analyzed for completeness, uniqueness, and consistency. It must meet the organization's defined "Quality Threshold."
3.  **Documentation:** Comprehensive metadata is attached, including definitions for every column, lineage information, and refresh schedules.
4.  **Security Review:** Access controls are verified to ensure the data is only available to authorized personnel and complies with privacy regulations (e.g., GDPR, CCPA).
5.  **Endorsement:** A formal sign-off from a Data Steward or Governance Committee.
6.  **Monitoring:** Continuous automated checks ensure the data remains within quality parameters.

## Common Patterns
*   **Tiered Certification:** Organizations often use levels (e.g., Bronze, Silver, Gold) to denote the degree of refinement and trust. "Gold" typically represents the Certified Data Set.
*   **Centralized Certification:** A central Data Governance Office (DGO) reviews all requests to ensure enterprise-wide consistency.
*   **Federated Certification:** Individual business units certify their own data according to a central framework, allowing for faster scaling in large organizations.

## Anti-Patterns
*   **Certification Inflation:** Certifying too many data sets, which dilutes the value of the "Certified" label and makes it difficult for users to find the true source of truth.
*   **The "Set and Forget" Mentality:** Certifying a data set once and never auditing it again. Business logic changes, and without re-validation, the certification becomes misleading.
*   **Shadow Certification:** When teams create their own "unofficial" certified lists outside of the governed catalog, leading to fragmentation.

> [!CAUTION]
> Avoid certifying data sets that have circular dependencies or are built upon uncertified, "black box" upstream sources. This creates a false sense of security.

## Edge Cases
*   **Real-Time/Streaming Data:** Certifying data that is constantly in motion is difficult. Certification in this context usually applies to the *schema* and the *pipeline logic* rather than a static snapshot of the data.
*   **Third-Party Data:** When an organization relies on external vendors, they cannot control the source. Certification here focuses on the validation of the data upon ingestion.
*   **Legacy Systems:** Systems with no documentation or known lineage. These often require "Reverse Engineering Certification," which is high-effort and high-risk.

## Related Topics
*   **Data Governance:** The overarching framework that defines the policies for certification.
*   **Data Quality Management (DQM):** The technical processes used to measure the metrics required for certification.
*   **Master Data Management (MDM):** The practice of creating a single master view of key business entities, which often results in Certified Data Sets.
*   **Data Cataloging:** The technology used to discover and display the certification status of data assets.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |