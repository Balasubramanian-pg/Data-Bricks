# 145. On Call Readiness for Data Teams

Canonical documentation for [145. On Call Readiness for Data Teams](Phase 6/145. On Call Readiness for Data Teams.md). This document defines concepts, terminology, and standard usage.

## Purpose
On-call readiness for data teams addresses the requirement for maintaining the reliability, availability, and integrity of data systems. Unlike traditional software engineering on-call, which focuses primarily on service uptime, data on-call readiness focuses on the continuous flow of accurate information through complex, often asynchronous pipelines. This topic exists to provide a framework for transitioning data operations from "best effort" to a structured, predictable response model that minimizes Mean Time to Detect (MTTD) and Mean Time to Resolve (MTTR) for data incidents.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative, focusing on the organizational and procedural requirements rather than specific software tooling.

## Scope
This documentation covers the organizational structures, procedural requirements, and technical prerequisites necessary to establish a functional on-call rotation within a data organization (Data Engineering, Analytics Engineering, or Data Science).

> [!IMPORTANT]
> **In scope:**
> * Incident lifecycle management for data pipelines and platforms.
> * Rotation design and health (burnout prevention).
> * Documentation standards (Runbooks/Playbooks).
> * Alerting logic and thresholding principles.
> * Post-incident analysis (Post-mortems).

> [!WARNING]
> **Out of scope:**
> * Specific vendor implementations (e.g., PagerDuty, Opsgenie configurations).
> * Specific data stack tutorials (e.g., how to fix a specific Spark error).
> * General IT helpdesk support unrelated to data infrastructure.

## Definitions
| Term | Definition |
|------|------------|
| Data Incident | An event where data is missing, delayed, duplicated, or incorrect, impacting downstream consumers. |
| Runbook | A standardized document providing step-by-step instructions for responding to a specific alert or failure. |
| Alert Fatigue | The desensitization of responders caused by a high volume of low-priority or non-actionable notifications. |
| Upstream Dependency | A data source or process that provides input to the system currently being monitored. |
| Backfill | The process of re-running data pipelines to correct or populate historical data after a failure or logic change. |
| SLO (Service Level Objective) | A target level of reliability for a data product (e.g., "99% of tables updated by 8:00 AM"). |

## Core Concepts

### The Stateful Nature of Data On-Call
Data on-call differs from service on-call due to statefulness. In software, a failing service can often be "fixed" by a restart or a rollback. In data, a failure often results in "poisoned" data or gaps in history that require complex remediation (backfilling) to restore the system to a correct state.

### Observability vs. Monitoring
Readiness requires moving beyond simple "up/down" monitoring. Data observability involves tracking:
* **Freshness:** Is the data arriving on time?
* **Distribution:** Does the data look the way it should (null rates, variance)?
* **Volume:** Did we receive the expected number of rows?
* **Schema:** Did an upstream change break the downstream contract?

> [!TIP]
> Think of monitoring as knowing *that* something is wrong, while observability is having enough context to know *why* it is wrong without having to write new queries.

### The "Actionable Alert" Principle
A core tenet of readiness is that every alert must be actionable. If an on-call engineer receives a notification that does not require immediate intervention, that notification is a "warning" or a "log," not an "alert."

## Standard Model

The standard model for Data On-Call Readiness follows a four-phase lifecycle:

1.  **Detection & Notification:** Automated systems identify a breach of SLOs and notify the designated responder based on a pre-defined rotation.
2.  **Triage & Communication:** The responder determines the severity (e.g., P0 for executive dashboards, P3 for internal staging) and notifies stakeholders of the impact.
3.  **Remediation:** The responder uses Runbooks to stabilize the system. This may involve pausing pipelines, rolling back code, or scaling resources.
4.  **Resolution & Backfill:** Once the root cause is addressed, the responder ensures data integrity by re-processing affected time windows.

> [!IMPORTANT]
> A responder is "Ready" only if they have the necessary permissions (IAM roles, database access) and the necessary documentation (Runbooks) to complete all four phases.

## Common Patterns

### Follow-the-Sun
Distributing on-call shifts across different time zones to ensure that responders are always working during their local daylight hours, reducing burnout.

### Primary and Secondary Responders
A two-tier rotation where the Primary handles the initial alert and the Secondary acts as a fallback or handles complex escalations.

### Triage Rotation (The "Data Warden")
A pattern where one person is dedicated to handling all incoming requests and minor alerts for a week, shielding the rest of the team from interruptions so they can focus on deep work.

## Anti-Patterns

### The "Hero" Culture
Relying on a single "expert" who knows how to fix everything but has not documented the process. This leads to a single point of failure and high turnover.

### Alerting on Upstream Failures
Configuring alerts for issues the data team cannot control (e.g., a third-party API being down) without a clear path for resolution. This contributes heavily to alert fatigue.

### Silent Failures
A lack of monitoring where pipelines "succeed" technically (exit code 0) but produce zero rows or corrupted data.

> [!CAUTION]
> Avoid circular dependencies in alerting. If your alerting system depends on the data pipeline it is monitoring, you will never receive a notification when the system is entirely down.

## Edge Cases

*   **The "Thundering Herd":** When a foundational upstream table fails, causing hundreds of downstream alerts to fire simultaneously. Readiness requires "alert grouping" or "dependency awareness" to identify the root cause.
*   **Schema Evolution:** An upstream source adds or drops a column. The pipeline may not "fail," but downstream models may produce incorrect results.
*   **Holiday/Peak Loading:** Seasonal spikes (e.g., Black Friday) may cause resource exhaustion that standard threshold alerts might not predict.

## Related Topics
*   **Data Quality Frameworks:** The technical checks that trigger on-call alerts.
*   **Data Contracts:** Agreements between producers and consumers to prevent unexpected breakages.
*   **Disaster Recovery (DR):** The high-level strategy for total system loss, of which on-call is a subset.
*   **Incident Post-Mortems:** The process of learning from failures to improve future readiness.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-26 | Initial AI-generated canonical documentation |