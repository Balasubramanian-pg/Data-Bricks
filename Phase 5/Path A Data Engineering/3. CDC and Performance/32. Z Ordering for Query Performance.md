# 032 Z Ordering for Query Performance

Canonical documentation for 032 Z Ordering for Query Performance. This document defines concepts, terminology, and standard usage.

## Purpose
Z-Ordering addresses the limitation of linear data sorting in multi-dimensional query patterns. In traditional data storage, data is typically sorted along a single dimension (e.g., by `timestamp`). While this optimizes queries filtered by that specific dimension, it provides no performance benefit for queries filtered by other attributes. 

Z-Ordering utilizes space-filling curves to map multi-dimensional data points into a one-dimensional layout while preserving spatial locality. This allows storage engines to perform efficient "data skipping" across multiple columns simultaneously, significantly reducing I/O overhead for complex analytical queries.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative.

## Scope
**In scope:**
* Mathematical foundations of the Morton Curve (Z-Order Curve).
* The relationship between data clustering and I/O efficiency.
* Criteria for selecting dimensions for Z-Ordering.
* Theoretical impact on data skipping and file pruning.

**Out of scope:**
* Specific syntax for cloud data warehouses or lakehouse formats (e.g., Delta Lake, Apache Iceberg).
* Hardware-specific optimization (e.g., SSD vs. HDD seek times).
* Comparison with other space-filling curves like Hilbert curves (except for context).

## Definitions
| Term | Definition |
|------|------------|
| **Z-Order Curve** | A space-filling curve that maps multidimensional data to one dimension while preserving the locality of the data points. Also known as Morton Order. |
| **Bit-Interleaving** | The process of interleaving the binary representations of coordinate values to produce a single Z-value. |
| **Data Skipping** | A query optimization technique where the engine avoids reading files or blocks that fall outside the range of the query filter based on stored metadata (min/max values). |
| **Locality** | The principle that data points close to each other in a multi-dimensional space should remain physically close in storage. |
| **Dimensionality** | The number of columns or attributes used to construct the Z-Order. |
| **Entropy** | In this context, the degree of randomness or distribution within a column that affects the efficiency of the Z-Order. |

## Core Concepts

### The Morton Curve
The Z-Order is generated by a mathematical function known as the Morton Curve. It works by taking the binary representation of multiple values (e.g., `X` and `Y` coordinates) and interleaving their bits. 
* Example: If `X = 5 (101)` and `Y = 3 (011)`, the interleaved Z-value is `100111` (taking the first bit of X, then the first bit of Y, and so on).

### Multi-Dimensional Clustering
Unlike a hierarchical sort (e.g., `ORDER BY col1, col2`), which prioritizes `col1` and only sorts `col2` within duplicate values of `col1`, Z-Ordering treats all specified columns with equal weight. This ensures that a filter on *any* of the Z-Ordered columns can effectively prune irrelevant data.

### I/O Reduction via Min/Max Metadata
Storage systems typically divide data into files or blocks and store the minimum and maximum values for each column within that block. Z-Ordering clusters similar values into the same blocks, narrowing the min/max ranges. When a query is executed, the engine compares the query predicate to these ranges; if the predicate does not overlap, the entire block is skipped.

## Standard Model

The standard model for implementing Z-Ordering for query performance follows a three-step lifecycle:

1.  **Dimension Selection:** Identifying high-cardinality columns that are frequently used together in query predicates (e.g., `customer_id` and `product_id`).
2.  **Interleaving and Sorting:** The system calculates the Z-value for each record based on the selected dimensions and sorts the records physically by this value.
3.  **Compaction:** Because Z-Ordering is most effective on large contiguous blocks of data, the model assumes a background or periodic process that rewrites smaller, fragmented files into larger, Z-Ordered files.

## Common Patterns

### High-Cardinality Filtering
Z-Ordering is most effective on columns with high cardinality (many unique values) that are frequently used in `WHERE` clauses. Common examples include `user_id`, `device_id`, or `geospatial_coordinates`.

### Temporal-Spatial Correlation
A common pattern involves Z-Ordering a `timestamp` column alongside a `location_id` or `category_id`. This supports queries that look for "events within a specific time window for a specific entity" without favoring one dimension over the other.

### Periodic Re-Clustering
Since data is often ingested in time-order, the Z-Order property degrades as new data arrives. A standard pattern is to run a "maintenance" or "optimize" job daily or weekly to re-apply Z-Ordering to the newly arrived data.

## Anti-Patterns

### Over-Dimensionality
Attempting to Z-Order too many columns (typically more than 4 or 5) leads to the "curse of dimensionality." As the number of dimensions increases, the effectiveness of the clustering in any single dimension drops exponentially, eventually making the Z-Order no more effective than a random distribution.

### Low-Cardinality Columns
Z-Ordering on columns with very few unique values (e.g., `Boolean` flags or `Gender`) provides negligible benefits. Standard partitioning or simple sorting is more efficient for these data types.

### Z-Ordering Frequently Updated Data
In systems where records are frequently updated or deleted, the overhead of maintaining the Z-Order can outweigh the query performance gains. Z-Ordering is best suited for immutable or append-heavy datasets.

## Edge Cases

### Data Skew
If one dimension has a significantly different distribution or range than others (e.g., one column ranges from 1–10 and another from 1–1,000,000), the bit-interleaving may favor the column with more bits, leading to lopsided clustering. Some implementations normalize values before interleaving to mitigate this.

### Null Values
The handling of `NULL` values in Z-Ordering is implementation-defined. Typically, `NULL`s are treated as the smallest or largest possible value, which can lead to "hotspots" at the beginning or end of the Z-Order curve, potentially reducing skipping efficiency for those blocks.

### Small Datasets
On datasets that fit entirely within a few storage blocks or in memory, the computational cost of calculating Z-values and rewriting the data exceeds any potential I/O savings.

## Related Topics
* **014 Data Skipping and Pruning:** The mechanism that consumes Z-Ordered data to improve performance.
* **022 Space-Filling Curves:** The broader mathematical family including Hilbert and Peano curves.
* **045 Partitioning Strategies:** A complementary technique for coarse-grained data organization.
* **089 Bloom Filters:** An alternative metadata-based skipping technique for point lookups.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-11 | Initial AI-generated canonical documentation |