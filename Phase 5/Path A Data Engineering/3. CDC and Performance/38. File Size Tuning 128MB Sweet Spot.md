# 038 File Size Tuning 128MB Sweet Spot

Canonical documentation for 038 File Size Tuning 128MB Sweet Spot. This document defines concepts, terminology, and standard usage.

## Purpose
The purpose of file size tuning is to optimize the balance between storage metadata management and computational throughput in distributed systems. The "128MB Sweet Spot" addresses the dual challenges of the "Small File Problem"—where excessive metadata overhead degrades system performance—and the "Large File Skew"—where excessively large files limit parallelism and increase recovery time. This topic establishes the theoretical basis for why 128MB serves as a heuristic baseline for general-purpose analytical workloads.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative.

## Scope
Clarify what is in scope and out of scope for this topic.

**In scope:**
* Theoretical foundations of file size optimization in distributed environments.
* The relationship between file size, metadata overhead, and I/O efficiency.
* Impact of file size on compute task granularity.
* Heuristics for determining optimal file sizes.

**Out of scope:**
* Specific vendor-specific configuration parameters (e.g., specific Spark or Hadoop properties).
* Hardware-level disk sector tuning.
* Network protocol-specific packet sizing.

## Definitions
Provide precise definitions for key terms.

| Term | Definition |
|------|------------|
| Small File Problem | A condition where a large number of files are significantly smaller than the system's block size, leading to excessive metadata and inefficient I/O. |
| Metadata Overhead | The computational and storage cost associated with managing file locations, permissions, and attributes rather than the data itself. |
| Throughput | The volume of data processed or transferred within a specific timeframe. |
| Parallelism | The ability of a system to execute multiple data processing tasks simultaneously across different segments of data. |
| Block Size | The nominal unit of data storage in a distributed file system; the atomic unit of data distribution. |
| Compaction | The process of merging multiple small files into a single larger file to improve read efficiency. |

## Core Concepts

### The Metadata-to-Data Ratio
Every file in a distributed system requires an entry in a metadata store (e.g., a NameNode or a Catalog). When files are too small, the time spent by the system looking up file locations and opening/closing file handles exceeds the time spent actually reading the data.

### I/O Efficiency and Seek Time
In traditional and cloud-based storage, there is a fixed latency cost associated with initiating a read operation. Larger files allow for sustained sequential reads, which maximize the bandwidth of the storage medium by amortizing the initial seek/connection latency over a larger volume of data.

### Compute Task Granularity
Distributed compute engines typically map one task to one file or one block. 
* **Too small:** High overhead for task scheduling and JVM/container startup.
* **Too large:** Reduced parallelism, leading to "stragglers" where one large task delays the entire job completion.

## Standard Model

The 128MB standard is derived from the historical evolution of distributed storage (moving from 64MB to 128MB and 256MB). It represents a "Goldilocks" zone for the following reasons:

1.  **Metadata Efficiency:** 128MB is large enough to keep the total number of files manageable for most metadata services even at petabyte scale.
2.  **Network Buffering:** It aligns well with standard network buffer sizes and TCP window scaling, allowing for efficient streaming over the wire.
3.  **Memory Management:** Most modern compute nodes can easily fit several 128MB blocks into memory simultaneously for processing (e.g., during joins or aggregations) without triggering Out-of-Memory (OOM) errors.
4.  **Compression Ratios:** Columnar formats (like Parquet or ORC) achieve better compression ratios when they have enough rows to build efficient dictionaries and bit-packing schemes, which 128MB typically provides.

## Common Patterns

### Compaction (Bin-Packing)
The practice of taking many small files (often generated by streaming or frequent micro-batching) and rewriting them into 128MB chunks. This is usually performed as a background maintenance task.

### Coalescing on Write
Configuring the data producer to buffer data in memory and only flush to storage once the 128MB threshold is reached, or using "shuffle" operations to redistribute data into fewer partitions before writing.

### Partition Alignment
Aligning file sizes within partitions so that each partition contains a multiple of 128MB. This ensures that when a partition is scanned, the compute engine can distribute the work evenly.

## Anti-Patterns

### Over-Partitioning
Creating a directory structure that is too granular (e.g., partitioning by `timestamp` down to the second), resulting in thousands of files that are only a few kilobytes in size.

### The "Single Large File" Trap
Storing an entire multi-terabyte dataset in a single file. While this minimizes metadata, it prevents parallel processing and makes the system vulnerable to single-point-of-failure delays during reads.

### Ignoring Compression
Measuring the "128MB" target based on uncompressed data. The "Sweet Spot" refers to the **on-disk, compressed size**, as this is what dictates I/O and metadata behavior.

## Edge Cases

### Extremely Wide Tables
For datasets with thousands of columns, a 128MB file may contain very few rows. In these cases, the overhead of the columnar headers and footers may necessitate increasing the target file size to 256MB or 512MB to maintain row-group efficiency.

### High-Latency Object Stores
In certain cloud object stores where the "Time to First Byte" is exceptionally high, the 128MB heuristic may be too low. Increasing the target to 256MB or 512MB can further amortize the high connection latency.

### Real-Time Requirements
In low-latency streaming applications, waiting to buffer 128MB of data may introduce unacceptable lag. In these scenarios, the "Small File Problem" is accepted as a trade-off for lower latency, with the expectation that a background compaction process will resolve the issue later.

## Related Topics
* **Columnar Storage Formats:** How Parquet and ORC interact with file sizing.
* **Data Partitioning Strategies:** The relationship between logical folders and physical files.
* **Distributed Metadata Management:** The architecture of services that track file locations.
* **Query Engine Optimization:** How engines like Trino, Presto, and Spark plan splits based on file size.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-11 | Initial AI-generated canonical documentation |