# 020 Deduplication Patterns

Canonical documentation for 020 Deduplication Patterns. This document defines concepts, terminology, and standard usage.

## Purpose
The 020 Deduplication Patterns address the fundamental challenge of data redundancy within distributed systems, data pipelines, and storage architectures. In environments where "at-least-once" delivery guarantees or asynchronous data ingestion are present, the same information may be transmitted or recorded multiple times. 

The purpose of these patterns is to ensure data integrity, optimize resource utilization (storage and compute), and maintain the "Single Source of Truth" by identifying and neutralizing redundant entries. This topic addresses the problem space of computational overhead, storage bloat, and downstream logic errors caused by duplicate data.

> [!NOTE]
> This documentation is intended to be implementation-agnostic and authoritative.

## Scope
Clarify what is in scope and out of scope for this topic.

**In scope:**
* Core logic for identifying redundant data (Identity functions).
* Temporal and spatial strategies for deduplication.
* Probabilistic vs. Deterministic approaches.
* Architectural placement of deduplication logic.

**Out of scope:**
* Specific vendor implementations (e.g., AWS SQS deduplication IDs, Kafka idempotent producers).
* File-system level block deduplication (e.g., ZFS/Btrfs internals).
* Data normalization (the process of restructuring relational databases).

## Definitions
| Term | Definition |
|------|------------|
| **Idempotency** | The property of an operation where it can be applied multiple times without changing the result beyond the initial application. |
| **Fingerprinting** | The process of generating a unique identifier (often a hash) from a data payload to represent its content. |
| **Deduplication Window** | The specific timeframe or buffer size within which a system tracks identifiers to detect duplicates. |
| **Deterministic Deduplication** | A method that guarantees 100% accuracy in identifying duplicates, usually requiring a stateful index of all seen keys. |
| **Probabilistic Deduplication** | A method that uses statistical structures (like Bloom Filters) to identify duplicates with a known margin of error, optimizing for memory over absolute certainty. |
| **Tombstoning** | The practice of marking a record as "deleted" or "processed" to prevent its re-ingestion or re-processing. |

## Core Concepts
### Identity and Determinism
At the heart of deduplication is the **Identity Function**. For any given piece of data, the system must derive a unique key. If two payloads produce the same key, they are considered duplicates. This key can be:
* **Natural:** Derived from existing data fields (e.g., Transaction ID, Email + Timestamp).
* **Synthetic:** Generated by the source system (e.g., UUID, Sequence Number).

### State Management
Deduplication is inherently stateful. To know if something has been seen before, the system must maintain a registry of "seen" identifiers. The management of this state—its persistence, TTL (Time-to-Live), and distribution—defines the efficiency of the pattern.

### Granularity
Deduplication can occur at different levels of the stack:
* **Entry-level:** Preventing the same record from entering a system.
* **Processing-level:** Allowing duplicates in storage but filtering them during compute/analysis.
* **Storage-level:** Merging identical records during background maintenance (compaction).

## Standard Model
The standard model for 020 Deduplication follows a three-stage pipeline:

1.  **Extraction/Fingerprinting:** The incoming payload is parsed, and a unique identifier is extracted or calculated.
2.  **Membership Testing:** The identifier is checked against a "Seen Index."
    *   If the identifier exists, the payload is flagged as a duplicate.
    *   If the identifier does not exist, it is added to the index.
3.  **Action/Resolution:** 
    *   *Drop:* The duplicate is discarded.
    *   *Update:* The existing record is updated with new metadata from the duplicate.
    *   *Ignore:* The duplicate is stored but marked as inactive.

## Common Patterns

### 1. The Bloom Filter (Probabilistic)
Used when memory is constrained and a small rate of false positives is acceptable. It allows for high-speed membership testing without storing the actual keys. If the filter says "no," the item is definitely new. If it says "yes," it *might* be a duplicate.

### 2. Sliding Window Deduplication
Common in stream processing. The system maintains a state of identifiers only for a specific duration (e.g., the last 24 hours). This prevents the "Seen Index" from growing infinitely.

### 3. Content-Based Hashing
The entire body of the data is hashed (e.g., SHA-256). This is used when there is no natural primary key, ensuring that identical payloads are treated as the same entity regardless of their source or arrival time.

### 4. Distributed Lock/Claim Check
In distributed systems, a worker "claims" an identifier in a global key-value store (like Redis) with an atomic "Set-if-not-exists" operation. If the operation fails, another worker is already processing that data.

## Anti-Patterns

*   **Global Locking:** Attempting to deduplicate by locking an entire database table. This leads to massive performance bottlenecks and negates the benefits of distributed processing.
*   **Non-Deterministic Key Generation:** Using timestamps or random values as part of a deduplication key, which results in the same data producing different keys upon retry.
*   **Infinite State Retention:** Failing to implement a TTL or cleanup strategy for the "Seen Index," eventually leading to memory exhaustion or storage overflow.
*   **Client-Side Only Deduplication:** Relying solely on the sender to not send duplicates. In distributed systems, network failures often cause the sender to retry, making server-side deduplication mandatory.

## Edge Cases

*   **Hash Collisions:** In probabilistic or hash-based models, two different payloads may generate the same fingerprint. While rare with 256-bit hashes, it must be accounted for in high-integrity systems.
*   **Late-Arriving Data:** In windowed deduplication, a duplicate may arrive after the window has closed and the state has been cleared. The system will incorrectly treat this as a new, unique record.
*   **Partial Updates:** Two records may have the same ID but different content (e.g., an updated status). Treating these as duplicates may result in data loss if the "Update" logic is not correctly implemented.
*   **Clock Skew:** In systems relying on temporal windows across multiple nodes, divergent system clocks can cause records to fall outside of deduplication windows inconsistently.

## Related Topics
*   **010 Idempotency Patterns:** The implementation of operations that can be safely retried.
*   **030 Event Sequencing:** Ensuring data is processed in the correct order, which often influences how duplicates are identified.
*   **045 Distributed State Management:** The underlying mechanisms for storing "Seen Indexes" across clusters.

## Change Log
| Version | Date | Description |
|---------|------|-------------|
| 1.0 | 2026-01-11 | Initial AI-generated canonical documentation |